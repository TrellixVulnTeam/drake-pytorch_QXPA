{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drake_in:  [<AutoDiffXd 0.0 nderiv=2> <AutoDiffXd 0.0 nderiv=2>]\n",
      "torch_in:  tensor([0., 0.], requires_grad=True)\n",
      "torch_out:  tensor([-0.2056], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.07312082604007633  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.13104201982836147  *  [0. 1.]\n",
      "putting into output:  -0.20559247671308717 ,  [ 0.07312083 -0.13104202]\n",
      "drake_in:  [<AutoDiffXd 2.73871029615e-07 nderiv=2>\n",
      " <AutoDiffXd -2.73868290904e-07 nderiv=2>]\n",
      "torch_in:  tensor([ 2.7387e-07, -2.7387e-07], requires_grad=True)\n",
      "torch_out:  tensor([-0.2056], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.0731208355216025  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.1310420400494838  *  [0. 1.]\n",
      "putting into output:  -0.20559242079915319 ,  [ 0.07312084 -0.13104204]\n",
      "drake_in:  [<AutoDiffXd -0.0638964231999 nderiv=2>\n",
      " <AutoDiffXd 0.114510691541 nderiv=2>]\n",
      "torch_in:  tensor([-0.0639,  0.1145], requires_grad=True)\n",
      "torch_out:  tensor([-0.2247], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.06967679638187571  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.12304664182575761  *  [0. 1.]\n",
      "putting into output:  -0.22471279891220905 ,  [ 0.0696768  -0.12304664]\n",
      "drake_in:  [<AutoDiffXd -0.3833785392 nderiv=2> <AutoDiffXd 0.687064149248 nderiv=2>]\n",
      "torch_in:  tensor([-0.3834,  0.6871], requires_grad=True)\n",
      "torch_out:  tensor([-0.3014], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.046091365945868504  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.08014021297782684  *  [0. 1.]\n",
      "putting into output:  -0.30144581264541953 ,  [ 0.04609137 -0.08014021]\n",
      "drake_in:  [<AutoDiffXd -0.958512597785 nderiv=2> <AutoDiffXd 1.68706414925 nderiv=2>]\n",
      "torch_in:  tensor([-0.9585,  1.6871], requires_grad=True)\n",
      "torch_out:  tensor([-0.3688], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.02805361216456464  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.02231413794438835  *  [0. 1.]\n",
      "putting into output:  -0.36883089636197863 ,  [ 0.02805361 -0.02231414]\n",
      "drake_in:  [<AutoDiffXd -1.6765226135 nderiv=2> <AutoDiffXd 2.12799737662 nderiv=2>]\n",
      "torch_in:  tensor([-1.6765,  2.1280], requires_grad=True)\n",
      "torch_out:  tensor([-0.3975], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.028459687191248487  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.018211274595407737  *  [0. 1.]\n",
      "putting into output:  -0.3974873892559282 ,  [ 0.02845969 -0.01821127]\n",
      "drake_in:  [<AutoDiffXd -5.26657269207 nderiv=2> <AutoDiffXd 4.33266351347 nderiv=2>]\n",
      "torch_in:  tensor([-5.2666,  4.3327], requires_grad=True)\n",
      "torch_out:  tensor([-0.4747], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.012708062023056261  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.02570871051218542  *  [0. 1.]\n",
      "putting into output:  -0.4746626899069093 ,  [-0.01270806 -0.02570871]\n",
      "drake_in:  [<AutoDiffXd -5.66010314735 nderiv=2> <AutoDiffXd 5.14958724592 nderiv=2>]\n",
      "torch_in:  tensor([-5.6601,  5.1496], requires_grad=True)\n",
      "torch_out:  tensor([-0.4878], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.009976947221982162  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.018068781361996272  *  [0. 1.]\n",
      "putting into output:  -0.4878017440925946 ,  [-0.00997695 -0.01806878]\n",
      "drake_in:  [<AutoDiffXd -6.18179786047 nderiv=2> <AutoDiffXd 6.75423457188 nderiv=2>]\n",
      "torch_in:  tensor([-6.1818,  6.7542], requires_grad=True)\n",
      "torch_out:  tensor([-0.5072], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.00825320075460442  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.014297366952290624  *  [0. 1.]\n",
      "putting into output:  -0.5072455594328859 ,  [-0.0082532  -0.01429737]\n",
      "drake_in:  [<AutoDiffXd -7.90144971818 nderiv=2> <AutoDiffXd 12.4817473605 nderiv=2>]\n",
      "torch_in:  tensor([-7.9014, 12.4817], requires_grad=True)\n",
      "torch_out:  tensor([-0.5272], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.028545890221053558  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  0.015445564906674182  *  [0. 1.]\n",
      "putting into output:  -0.5271822802512124 ,  [0.02854589 0.01544556]\n",
      "drake_in:  [<AutoDiffXd -9.00745168315 nderiv=2> <AutoDiffXd 12.526268511 nderiv=2>]\n",
      "torch_in:  tensor([-9.0075, 12.5263], requires_grad=True)\n",
      "torch_out:  tensor([-0.5491], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.011138007953027867  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  0.004422996084422229  *  [0. 1.]\n",
      "putting into output:  -0.549086296826206 ,  [0.01113801 0.004423  ]\n",
      "drake_in:  [<AutoDiffXd -10.7090337205 nderiv=2> <AutoDiffXd 14.1138474367 nderiv=2>]\n",
      "torch_in:  tensor([-10.7090,  14.1138], requires_grad=True)\n",
      "torch_out:  tensor([-0.5578], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.002675835609246832  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  -0.0008888325882148917  *  [0. 1.]\n",
      "putting into output:  -0.5577624134643424 ,  [ 0.00267584 -0.00088883]\n",
      "drake_in:  [<AutoDiffXd -13.8303998439 nderiv=2> <AutoDiffXd 17.9636124896 nderiv=2>]\n",
      "torch_in:  tensor([-13.8304,  17.9636], requires_grad=True)\n",
      "torch_out:  tensor([-0.5665], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.002198290920445931  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  0.00012666333164208644  *  [0. 1.]\n",
      "putting into output:  -0.5664621000196104 ,  [0.00219829 0.00012666]\n",
      "drake_in:  [<AutoDiffXd -17.5837888792 nderiv=2> <AutoDiffXd 22.4753349421 nderiv=2>]\n",
      "torch_in:  tensor([-17.5838,  22.4753], requires_grad=True)\n",
      "torch_out:  tensor([-0.5723], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.0015190044268614707  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  0.00030110671820894017  *  [0. 1.]\n",
      "putting into output:  -0.5722553882047985 ,  [0.001519   0.00030111]\n",
      "drake_in:  [<AutoDiffXd -22.4781280008 nderiv=2> <AutoDiffXd 28.312450232 nderiv=2>]\n",
      "torch_in:  tensor([-22.4781,  28.3125], requires_grad=True)\n",
      "torch_out:  tensor([-0.5765], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.0009072944979406836  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  0.00023531037415048647  *  [0. 1.]\n",
      "putting into output:  -0.5764616322588662 ,  [0.00090729 0.00023531]\n",
      "drake_in:  [<AutoDiffXd -28.2282004546 nderiv=2> <AutoDiffXd 35.1563447561 nderiv=2>]\n",
      "torch_in:  tensor([-28.2282,  35.1563], requires_grad=True)\n",
      "torch_out:  tensor([-0.5791], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.0005006759706112209  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  0.00014196741808985208  *  [0. 1.]\n",
      "putting into output:  -0.5791131794562248 ,  [0.00050068 0.00014197]\n",
      "drake_in:  [<AutoDiffXd -34.6812831399 nderiv=2> <AutoDiffXd 42.8337307831 nderiv=2>]\n",
      "torch_in:  tensor([-34.6813,  42.8337], requires_grad=True)\n",
      "torch_out:  tensor([-0.5807], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.00028032214743483594  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  8.883912502627042e-05  *  [0. 1.]\n",
      "putting into output:  -0.5806844368159904 ,  [2.80322147e-04 8.88391250e-05]\n",
      "drake_in:  [<AutoDiffXd -41.8513832454 nderiv=2> <AutoDiffXd 51.3615641824 nderiv=2>]\n",
      "torch_in:  tensor([-41.8514,  51.3616], requires_grad=True)\n",
      "torch_out:  tensor([-0.5816], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  0.00016448855382681526  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  6.133145353473466e-05  *  [0. 1.]\n",
      "putting into output:  -0.5816024387250469 ,  [1.64488554e-04 6.13314535e-05]\n",
      "drake_in:  [<AutoDiffXd -49.7474799229 nderiv=2> <AutoDiffXd 60.7501550863 nderiv=2>]\n",
      "torch_in:  tensor([-49.7475,  60.7502], requires_grad=True)\n",
      "torch_out:  tensor([-0.5821], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  9.87686437221894e-05  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  4.3172814231127664e-05  *  [0. 1.]\n",
      "putting into output:  -0.5821303586693053 ,  [9.87686437e-05 4.31728142e-05]\n",
      "drake_in:  [<AutoDiffXd -58.2343575924 nderiv=2> <AutoDiffXd 70.8391251848 nderiv=2>]\n",
      "torch_in:  tensor([-58.2344,  70.8391], requires_grad=True)\n",
      "torch_out:  tensor([-0.5824], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  5.883619881209624e-05  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  2.909058412534304e-05  *  [0. 1.]\n",
      "putting into output:  -0.5824230422467109 ,  [5.88361988e-05 2.90905841e-05]\n",
      "drake_in:  [<AutoDiffXd -67.1091863282 nderiv=2> <AutoDiffXd 81.3880240449 nderiv=2>]\n",
      "torch_in:  tensor([-67.1092,  81.3880], requires_grad=True)\n",
      "torch_out:  tensor([-0.5826], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  3.427922935080159e-05  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  1.8513007582795127e-05  *  [0. 1.]\n",
      "putting into output:  -0.5825789739538161 ,  [3.42792294e-05 1.85130076e-05]\n",
      "drake_in:  [<AutoDiffXd -76.1981500357 nderiv=2> <AutoDiffXd 92.1907782148 nderiv=2>]\n",
      "torch_in:  tensor([-76.1982,  92.1908], requires_grad=True)\n",
      "torch_out:  tensor([-0.5827], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  1.9528931890513232e-05  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  1.1236075933348278e-05  *  [0. 1.]\n",
      "putting into output:  -0.5826596191089767 ,  [1.95289319e-05 1.12360759e-05]\n",
      "drake_in:  [<AutoDiffXd -85.3954039977 nderiv=2> <AutoDiffXd 103.121902559 nderiv=2>]\n",
      "torch_in:  tensor([-85.3954, 103.1219], requires_grad=True)\n",
      "torch_out:  tensor([-0.5827], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  1.0925556377401483e-05  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  6.587379550490858e-06  *  [0. 1.]\n",
      "putting into output:  -0.5827006248998863 ,  [1.09255564e-05 6.58737955e-06]\n",
      "drake_in:  [<AutoDiffXd -94.6462336654 nderiv=2> <AutoDiffXd 114.116535358 nderiv=2>]\n",
      "torch_in:  tensor([-94.6462, 114.1165], requires_grad=True)\n",
      "torch_out:  tensor([-0.5827], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  6.0280562563947534e-06  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  3.7673439067394387e-06  *  [0. 1.]\n",
      "putting into output:  -0.582721298233162 ,  [6.02805626e-06 3.76734391e-06]\n",
      "drake_in:  [<AutoDiffXd -103.924907598 nderiv=2> <AutoDiffXd 125.144178929 nderiv=2>]\n",
      "torch_in:  tensor([-103.9249,  125.1442], requires_grad=True)\n",
      "torch_out:  tensor([-0.5827], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  3.2904367666261416e-06  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  2.1156710412583995e-06  *  [0. 1.]\n",
      "putting into output:  -0.5827316796394284 ,  [3.29043677e-06 2.11567104e-06]\n",
      "drake_in:  [<AutoDiffXd -113.219767678 nderiv=2> <AutoDiffXd 136.191019253 nderiv=2>]\n",
      "torch_in:  tensor([-113.2198,  136.1910], requires_grad=True)\n",
      "torch_out:  tensor([-0.5827], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  1.7808431017767611e-06  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  1.1717561496570282e-06  *  [0. 1.]\n",
      "putting into output:  -0.5827368835151308 ,  [1.78084310e-06 1.17175615e-06]\n",
      "drake_in:  [<AutoDiffXd -113.219767678 nderiv=2> <AutoDiffXd 136.191019253 nderiv=2>]\n",
      "torch_in:  tensor([-113.2198,  136.1910], requires_grad=True)\n",
      "torch_out:  tensor([-0.5827], grad_fn=<AddBackward0>)\n",
      "\n",
      "iter j:  0\n",
      "dy_jdu_a[i] * u_i_deriv =  1.7808431017767611e-06  *  [1. 0.]\n",
      "dy_jdu_a[i] * u_i_deriv =  1.1717561496570282e-06  *  [0. 1.]\n",
      "putting into output:  -0.5827368835151308 ,  [1.78084310e-06 1.17175615e-06]\n",
      "SolutionResult.kSolutionFound SolutionResult.kSolutionFound\n",
      "False\n",
      "[-113.21976768  136.19101925]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, absolute_import\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "import warnings\n",
    "\n",
    "import pydrake\n",
    "from pydrake.all import (\n",
    "    SolutionResult\n",
    ")\n",
    "import pydrake.symbolic as sym\n",
    "from pydrake.solvers import mathematicalprogram as mp\n",
    "from pydrake.solvers.mathematicalprogram import SolverType\n",
    "import pydrake.autodiffutils\n",
    "from pydrake.autodiffutils import AutoDiffXd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "prog = mp.MathematicalProgram()\n",
    "x = prog.NewContinuousVariables(2, \"x\")\n",
    "# prog.AddLinearConstraint(x[0] >= 1)\n",
    "# prog.AddLinearConstraint(x[1] >= 1)\n",
    "# prog.AddQuadraticCost(np.eye(2), np.zeros(2), x)\n",
    "# Redundant cost just to check the spelling.\n",
    "# prog.AddQuadraticErrorCost(vars=x, Q=np.eye(2),\n",
    "#                            x_desired=np.zeros(2))\n",
    "# prog.AddL2NormCost(A=np.eye(2), b=np.zeros(2), vars=x)\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self, layer_norm=False):\n",
    "        super(FC, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_norm=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer_norm = layer_norm\n",
    "\n",
    "        self.l1 = nn.Linear(2, 64)\n",
    "        self.ln1 = nn.LayerNorm(64)\n",
    "        self.tanh1 = F.tanh\n",
    "        self.l2 = nn.Linear(64, 64)\n",
    "        self.ln2 = nn.LayerNorm(64)\n",
    "        self.tanh2 = F.tanh\n",
    "        self.l3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        if self.layer_norm: x = self.ln1(x)\n",
    "        x = self.tanh1(x)\n",
    "        x = self.l2(x)\n",
    "        if self.layer_norm: x = self.ln2(x)\n",
    "        x = self.tanh2(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "# net = FC()\n",
    "net = MLP()\n",
    "\n",
    "# This function uses an neural network, but only has one output...\n",
    "def autodiffable_function_nn(drake_in):\n",
    "    global net\n",
    "\n",
    "    # Convert input to a torch tensor\n",
    "    print(\"drake_in: \", drake_in)\n",
    "    n_inputs = drake_in.shape[0]\n",
    "    just_values = np.array([drake_in[i].value() for i in range(drake_in.shape[0])])\n",
    "    torch_in = torch.tensor(just_values, requires_grad=True)\n",
    "    print(\"torch_in: \", torch_in)\n",
    "                                                                                                                                                                                                                                                       \n",
    "    # Run the forward pass.\n",
    "    # We'll do the backward pass(es) when we calculate output and it's gradients.\n",
    "    torch_out = net.forward(torch_in)\n",
    "    print(\"torch_out: \", torch_out)\n",
    "    \n",
    "    # Currently we only support one output\n",
    "    assert torch_out.shape[0] == 1, \"Need just one output for valid cost function\"\n",
    "    \n",
    "    # Do derivative calculation and pack into the output vector.\n",
    "    # Because neural network might have multiple outputs, I can't simply use net.backward() with no argument.\n",
    "    # Instead need to follow the advice here:\n",
    "    #     https://discuss.pytorch.org/t/clarification-using-backward-on-non-scalars/1059\n",
    "    n_outputs = torch_out.shape[0]\n",
    "    drake_out = [0]*n_outputs\n",
    "    for j in range(n_outputs):\n",
    "        print(\"\\niter j: \", j)\n",
    "        y_j_value = torch_out[j].clone().detach().numpy()\n",
    "        # Equation: y.derivs = dydu*u.derivs() + dydp*p.derivs()\n",
    "        # Alternate equation, for each y, y_j.deriv = sum_i  (dy_jdu[i] * u[i].deriv)\n",
    "        #                          (#y's) (1x#derivs) (#u's)  (1x#u's)    (1x#derivs)\n",
    "\n",
    "        # Make empty accumulator\n",
    "        y_j_deriv = np.zeros_like(drake_in[0].derivatives())\n",
    "        \n",
    "        # https://discuss.pytorch.org/t/clarification-using-backward-on-non-scalars/1059\n",
    "        output_selector = torch.zeros(1, n_outputs)\n",
    "        output_selector[j] = 1.0 # Set the output we want a derivative w.r.t. to 1.\n",
    "        torch_out.backward(output_selector, retain_graph=True)\n",
    "        dy_jdu = torch_in.grad.numpy() # From Torch, give us back a numpy object\n",
    "        for i in range(n_inputs):\n",
    "            u_i_deriv = drake_in[i].derivatives()\n",
    "            print(\"dy_jdu_a[i] * u_i_deriv = \", dy_jdu[i], \" * \",  u_i_deriv)\n",
    "            y_j_deriv += dy_jdu[i] * u_i_deriv;\n",
    "        print(\"putting into output: \", y_j_value, \", \", y_j_deriv)\n",
    "        drake_out[j] = AutoDiffXd(y_j_value, y_j_deriv)\n",
    "    return drake_out[0]\n",
    "\n",
    "def autodiffable_function_nn(drake_in):\n",
    "    \n",
    "\n",
    "def autodiffable_function_simple(x):\n",
    "    print(type(x), x.shape)\n",
    "    print(type(x[0]), x.shape)\n",
    "    return (x[0]-2.)*(x[1]-2.)\n",
    "\n",
    "# prog.AddCost(autodiffable_function_simple, x)\n",
    "prog.AddCost(autodiffable_function_nn, x)\n",
    "\n",
    "result = prog.Solve()\n",
    "print(result, mp.SolutionResult.kSolutionFound)\n",
    "\n",
    "x_expected = np.array([1, 1])\n",
    "print(np.allclose(prog.GetSolution(x), x_expected))\n",
    "\n",
    "print(prog.GetSolution(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydrake.systems.scalar_conversion import TemplateSystem\n",
    "from pydrake.autodiffutils import AutoDiffXd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pydrake.autodiffutils.AutoDiffXd'>\n",
      "0.0\n",
      "[1. 0.]\n",
      "<class 'pydrake.autodiffutils.AutoDiffXd'>\n",
      "1.8258068641e-07\n",
      "[1. 0.]\n",
      "<class 'pydrake.autodiffutils.AutoDiffXd'>\n",
      "0.514568548895\n",
      "[1. 0.]\n",
      "<class 'pydrake.autodiffutils.AutoDiffXd'>\n",
      "0.257284274447\n",
      "[1. 0.]\n",
      "<class 'pydrake.autodiffutils.AutoDiffXd'>\n",
      "0.488616822259\n",
      "[1. 0.]\n",
      "<class 'pydrake.autodiffutils.AutoDiffXd'>\n",
      "0.255438203943\n",
      "[1. 0.]\n",
      "<class 'pydrake.autodiffutils.AutoDiffXd'>\n",
      "0.292010091187\n",
      "[1. 0.]\n",
      "<class 'pydrake.autodiffutils.AutoDiffXd'>\n",
      "-2.29201009119\n",
      "[1. 0.]\n",
      "<class 'pydrake.autodiffutils.AutoDiffXd'>\n",
      "0.291948908967\n",
      "[1. 0.]\n",
      "<class 'pydrake.autodiffutils.AutoDiffXd'>\n",
      "0.292892542544\n",
      "[1. 0.]\n",
      "<class 'pydrake.autodiffutils.AutoDiffXd'>\n",
      "0.292892542544\n",
      "[1. 0.]\n",
      "(SolutionResult.kInfeasibleConstraints, SolutionResult.kSolutionFound)\n",
      "True\n",
      "(array([0.29289254, 0.29289264]), array([0.]))\n"
     ]
    }
   ],
   "source": [
    "# Set Up Mathematical Program\n",
    "prog = mp.MathematicalProgram()\n",
    "x = prog.NewContinuousVariables(2, \"x\")\n",
    "z = prog.NewContinuousVariables(1, \"z\")\n",
    "# print(type(x[0]), type(z[0]))\n",
    "prog.AddCost(z[0])\n",
    "\n",
    "# Add LorentzConeConstraints\n",
    "prog.AddLorentzConeConstraint(np.array([0*x[0]+1, x[0]-1, x[1]-1]))\n",
    "prog.AddLorentzConeConstraint(np.array([z[0], x[0], x[1]]))\n",
    "\n",
    "def autodiffable_function(x):\n",
    "    print(type(x[0]))\n",
    "    print(x[0].value())\n",
    "    print(x[0].derivatives())\n",
    "    return (x[0]-2.)*(x[1]-2.)\n",
    "prog.AddCost(autodiffable_function, x)\n",
    "\n",
    "# Test result\n",
    "result = prog.Solve()\n",
    "print(result, mp.SolutionResult.kSolutionFound)\n",
    "\n",
    "# Check answer\n",
    "x_expected = np.array([1-2**(-0.5), 1-2**(-0.5)])\n",
    "print(np.allclose(prog.GetSolution(x), x_expected))\n",
    "\n",
    "print(prog.GetSolution(x), prog.GetSolution(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "x = torch.tensor([[2,1]], dtype=torch.float32, requires_grad=True)\n",
    "M = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
    "y = torch.mm(x, M)\n",
    "jacobian = torch.zeros((2, 2))\n",
    "y.backward(torch.tensor([[1., 0.]], dtype=torch.float32), retain_graph=True)\n",
    "jacobian[:,0] = x.grad.data\n",
    "x.grad.data.zero_()\n",
    "y.backward(torch.tensor([[0, 1]], dtype=torch.float32), retain_graph=True)\n",
    "jacobian[:,1] = x.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -2.],\n",
       "        [ 2.,  2.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
    "out = x.pow(2).sum()\n",
    "out.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AutoDiffXd 1.0 nderiv=2>],\n",
       "       [<AutoDiffXd 1.0 nderiv=2>]], dtype=object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autodiff_in = np.array([\n",
    "    [AutoDiffXd(1.0, [1.0, 0.0]),],\n",
    "    [AutoDiffXd(1.0, [1.0, 0.0]),]\n",
    "])\n",
    "autodiff_in\n",
    "# type(autodiff_in), autodiff_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([\n",
    "    [1., 2.],\n",
    "    [3., 4.]\n",
    "])\n",
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (2,))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autodiff_in = np.array(\n",
    "    [AutoDiffXd(1.0, [1.0, 0.0]), AutoDiffXd(1.0, [1.0, 0.0])]\n",
    ").reshape()\n",
    "autodiff_in\n",
    "type(autodiff_in), autodiff_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "test = np.array([AutoDiffXd(x) for x in q])\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
